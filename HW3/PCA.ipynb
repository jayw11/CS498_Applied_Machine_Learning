{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# HW3 (Textbook 5.10)\n",
    "### Group members: Haipu Sun, Jie Wen, Changcheng Fu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df = pd.read_csv(\"./iris.data\", header = None)\n",
    "df = df.drop([4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data smoothed with PCA (no noise)\n",
      "1  principal components:  0.08553854253383855\n",
      "2  principal components:  0.025381388913005544\n",
      "3  principal components:  0.005881285069623817\n",
      "4  principal components:  1.7455858643948151e-31\n"
     ]
    }
   ],
   "source": [
    "print(\"original data smoothed with PCA (no noise)\")\n",
    "for n in range(1,5):\n",
    "    model = PCA(n_components = n)\n",
    "    model.fit(df)\n",
    "    low_dim = model.transform(df)\n",
    "    df_predict = model.inverse_transform(low_dim)\n",
    "    mse_val = mean_squared_error(df, df_predict)\n",
    "    print(n, \" principal components: \", mse_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### From the mse values above, we can see that when smoothing original data (no noise), using more principal components gives more accurate estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Q(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With std of  0.1  mse values (from 1 to 4):  [0.08784574731001296, 0.029983844397522785, 0.013288966868507966, 0.010435071498165692]\n",
      "With std of  0.2  mse values (from 1 to 4):  [0.09513750310837024, 0.04623193964710114, 0.036129669487087845, 0.03953783354614171]\n",
      "With std of  0.5  mse values (from 1 to 4):  [0.1522565455474919, 0.15637640488233157, 0.19901191283566227, 0.24578772441028846]\n",
      "With std of  1  mse values (from 1 to 4):  [0.3460576774959615, 0.5563899758726005, 0.7700175180434939, 0.9281588335532656]\n"
     ]
    }
   ],
   "source": [
    "mu, sigma = 0,[0.1, 0.2, 0.5, 1]\n",
    "n_list = [1,2,3,4]\n",
    "mse_vals = []\n",
    "for sig in sigma:\n",
    "    iris_data = df.copy()\n",
    "    for i in iris_data.columns:\n",
    "        iris_data[i] = iris_data[i].add(np.random.normal(mu, sig, 150))\n",
    "    temp = []\n",
    "    for n in n_list:\n",
    "        model = PCA(n_components = n)\n",
    "        model.fit(iris_data)\n",
    "        low_dim = model.transform(iris_data)\n",
    "        df_predict = model.inverse_transform(low_dim)\n",
    "        mse = mean_squared_error(df, df_predict)\n",
    "        temp.append(mse)\n",
    "    mse_vals.append(temp)\n",
    "for i in range(4):\n",
    "    print(\"With std of \", sigma[i], \" mse values (from 1 to 4): \", mse_vals[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### From the result printed above, we can see that as the noise increase (using larger std to compute noise value), applying fewer principal components gives more accurate estimate of the original result (with smaller mse value). The reason why it's opposite to the result of smoothing original data set is that when we smooth original data set, the more principal components we keep, more information is kept in the smoothed one; however, after smoothing the data set with noise, keeping more principal components also means keeping more noise, so as the noise increase, fewer principal components we use, less noise the smooth data has so more accurate our estimate is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Q(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With  10 of zeros mse values (from 1 to 4):  [0.12566661321964406, 0.23293518644599723, 0.22936112505883502, 0.23116666666666677]\n",
      "With  20 of zeros mse values (from 1 to 4):  [0.32585902221177365, 0.6254172225039724, 0.6332851480365186, 0.6580499999999999]\n",
      "With  30 of zeros mse values (from 1 to 4):  [0.3449150059624686, 0.6411800116192715, 0.6909299169015035, 0.7152]\n",
      "With  40 of zeros mse values (from 1 to 4):  [0.5531624924041999, 0.9571005639359149, 1.0476612195970654, 1.0668000000000004]\n"
     ]
    }
   ],
   "source": [
    "n, w = 600, [10, 20, 30, 40]\n",
    "n_list = [1,2,3,4]\n",
    "mse_vals = []\n",
    "for ww in w:\n",
    "    p = 1 - ww/600\n",
    "    #print(p)\n",
    "    iris_data = df.copy()\n",
    "    for i in iris_data.columns:\n",
    "        iris_data[i] = iris_data[i].mul(np.random.binomial(1, p, 150))\n",
    "    #print(600 - np.count_nonzero(iris_data.to_numpy()))\n",
    "    temp = []\n",
    "    for n in n_list:\n",
    "        model = PCA(n_components = n)\n",
    "        model.fit(iris_data)\n",
    "        low_dim = model.transform(iris_data)\n",
    "        df_predict = model.inverse_transform(low_dim)\n",
    "        mse = mean_squared_error(df, df_predict)\n",
    "        temp.append(mse)\n",
    "    mse_vals.append(temp)\n",
    "for i in range(4):\n",
    "    print(\"With \", w[i] ,\"of zeros mse values (from 1 to 4): \", mse_vals[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### From the result printed above, we can see that as the noise increase (using larger std to compute noise value), applying fewer principal components gives more accurate estimate of the original result (with smaller mse value). The reason why it happens is the same as Q(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
